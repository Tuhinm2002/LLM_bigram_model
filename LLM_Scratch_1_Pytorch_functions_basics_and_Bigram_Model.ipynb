{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGI5l5oHwpIaU/t+mnDnkD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tuhinm2002/LLM_bigram_model/blob/main/LLM_Scratch_1_Pytorch_functions_basics_and_Bigram_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ],
      "metadata": {
        "id": "ylttNrerzrer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "4sRergy6yAI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pKbYaTYTvJxz",
        "outputId": "27201bf4-2fe0-4ef8-dbb6-4113a7efff9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3])\n",
            "torch.int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "tensor = torch.tensor([1,2,3])\n",
        "print(tensor),print(tensor.dtype)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define a probability function"
      ],
      "metadata": {
        "id": "5BeC1RaS0ER8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "proba = torch.tensor([0.4,0.4,0.2]) # both or all should add up to 100 % although works otherwise\n",
        "# proba = torch.tensor([0.4,0.4,0.3])\n",
        "samples = torch.multinomial(proba,num_samples=10,replacement=True)\n",
        "print(samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2atQfrJ9zzGF",
        "outputId": "d80190d1-01e2-4c0b-9df4-f86d939db1ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2, 0, 0, 2, 0, 0, 1, 0, 2, 0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = torch.tensor([1,2,3])\n",
        "out = torch.cat((tensor,torch.tensor([4])),dim=0)\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bc95agvZ2BeJ",
        "outputId": "e2f09dac-055e-463c-a585-6b3c99b7bba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.tril(torch.ones(5,5))\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVauB7263dAf",
        "outputId": "21710208-0005-4604-f96e-2d0651ba0fa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.triu(torch.ones(5,5))\n",
        "print(out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rGu6KTs5Ktq",
        "outputId": "75fc63c7-7425-4d6d-f293-ef7a6b0d5364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1., 1., 1., 1.],\n",
            "        [0., 1., 1., 1., 1.],\n",
            "        [0., 0., 1., 1., 1.],\n",
            "        [0., 0., 0., 1., 1.],\n",
            "        [0., 0., 0., 0., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "out = torch.zeros(5,5).masked_fill(torch.tril(torch.ones(5,5)) == 0,float(\"-inf\"))\n",
        "print(out)\n",
        "print(\"\")\n",
        "print(torch.exp(out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q24YTFiI5QcY",
        "outputId": "9ca3b5cd-e547-4dd1-d119-fbda926b6fc2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0., -inf, -inf, -inf, -inf],\n",
            "        [0., 0., -inf, -inf, -inf],\n",
            "        [0., 0., 0., -inf, -inf],\n",
            "        [0., 0., 0., 0., -inf],\n",
            "        [0., 0., 0., 0., 0.]])\n",
            "\n",
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1.]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`torch.nn` module for learning and training\n",
        "using weights and bias"
      ],
      "metadata": {
        "id": "2JM2MNrU9A1r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "linear = nn.Linear(4,4,bias=False) # 4,4 is input and output feature\n",
        "print(linear)\n",
        "print(\"\")\n",
        "print(linear(sample))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "op4MMXFz7vQm",
        "outputId": "c992a725-caec-4ed3-c997-199cc4d0ee55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=4, out_features=4, bias=False)\n",
            "\n",
            "tensor([-2.0740, -2.4534, -0.9538, -3.3229], grad_fn=<SqueezeBackward4>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "softmax function :\n",
        "e^1 = 2.71\n",
        "therefore,`softmax(torch.tensor([1,2,3,4]))` will be `([2.71^1,2.71^2,2.71^3,2.71^4])/sum(([2.71^1,2.71^2,2.71^3,2.71^4])`\n",
        "this will be for each value\n",
        "i.e. `2.71^1/sum(([2.71^1,2.71^2,2.71^3,2.71^4]))` = 0.0321"
      ],
      "metadata": {
        "id": "Weltl9obBvu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "tensor1 = torch.tensor([1,2,3,4],dtype=torch.float32)\n",
        "print(F.softmax(tensor1,dim=0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cae280Cz9X7N",
        "outputId": "aa57ac37-9e8b-4994-883c-87bccdc1e21d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0321, 0.0871, 0.2369, 0.6439])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Embedding layer"
      ],
      "metadata": {
        "id": "5TH8-7q4JwA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "embedding_dim = 100\n",
        "embedding = nn.Embedding(vocab_size,embedding_dim)\n",
        "\n",
        "# Input tensors\n",
        "tensor2 = torch.tensor([1,2,3,4])\n",
        "out = embedding(tensor2)\n",
        "print(out)\n",
        "print(\"\")\n",
        "print(\"\")\n",
        "print(out.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fuf1_HD2Jvai",
        "outputId": "4c340297-4ffb-4a8b-c97e-4bf6764e22ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5.6306e-01,  1.4630e+00,  1.3142e-01,  8.5268e-01,  6.5743e-01,\n",
            "          1.0793e+00, -5.5240e-01,  1.3192e+00,  6.6005e-02,  6.3343e-01,\n",
            "         -6.3350e-01, -9.3992e-01,  6.2123e-01,  1.6578e-01,  2.3429e-01,\n",
            "         -1.0593e+00,  8.5137e-01,  1.2305e+00,  1.0919e+00, -2.5327e+00,\n",
            "         -2.1678e-01,  5.0313e-01,  4.0254e-01, -7.4746e-01, -7.4544e-01,\n",
            "         -8.2496e-02, -1.3704e+00,  4.2273e-01,  1.2805e+00,  4.2276e-01,\n",
            "          9.2660e-01,  1.2119e-01,  6.2055e-01, -1.1986e+00,  1.0113e+00,\n",
            "          4.9958e-01, -1.6110e+00, -8.2986e-01,  5.8413e-01, -2.6250e-01,\n",
            "          6.4469e-01, -2.3336e-01, -3.8456e-01,  5.2955e-02,  4.4865e-01,\n",
            "         -2.5052e-01, -3.0341e-01, -7.4268e-01,  7.5896e-02, -5.1916e-01,\n",
            "          1.0074e+00,  1.1117e+00,  3.4136e-01,  6.3872e-01, -7.9072e-01,\n",
            "          2.1519e-01,  1.6158e-01, -7.5772e-01,  3.4995e-03,  1.3306e+00,\n",
            "         -7.0625e-01, -1.2438e+00, -7.5816e-01, -1.5858e-01,  2.0923e+00,\n",
            "          4.3545e-01,  6.1532e-01, -2.2294e-01,  1.4175e+00, -6.5182e-01,\n",
            "         -3.4180e-01, -2.8696e-01, -4.4276e-01, -2.3505e-01, -1.6623e+00,\n",
            "          1.4235e+00, -5.6948e-01, -7.8783e-01,  3.4382e-01,  8.1581e-02,\n",
            "          7.2053e-01,  2.2600e+00, -1.7166e-02,  9.1785e-01,  7.6789e-01,\n",
            "          3.9419e-01,  2.7732e-01, -1.7869e+00, -1.6829e-01,  4.5539e-01,\n",
            "         -8.3078e-01,  2.4600e-01,  2.4786e-01,  6.1660e-01, -1.4999e+00,\n",
            "          1.4036e+00, -2.0505e+00,  1.2893e+00, -7.2297e-01,  1.5200e-01],\n",
            "        [ 7.7257e-02, -1.1645e+00, -3.8748e-01,  1.0886e+00, -1.6686e+00,\n",
            "          6.8262e-01, -2.0400e-01,  3.0515e-02, -1.3370e+00, -1.8828e+00,\n",
            "         -1.7825e-01, -5.6759e-01,  8.2481e-02,  3.2064e-01, -2.2582e-01,\n",
            "         -2.7311e-01, -5.4266e-01, -7.6904e-01,  6.4831e-01, -6.8225e-01,\n",
            "          9.0301e-02,  5.4261e-01,  1.4297e+00, -8.6193e-02,  7.3205e-01,\n",
            "         -8.7037e-01, -5.5725e-01, -7.9758e-01,  1.1599e+00, -3.2363e-01,\n",
            "         -6.8336e-01, -1.0221e+00, -5.9746e-01, -8.1569e-01,  7.0133e-01,\n",
            "          3.6116e-01, -3.4971e-01, -9.9525e-01, -2.4001e+00, -2.2249e-01,\n",
            "         -3.1649e-01,  1.4545e+00,  5.0526e-01,  1.4651e+00,  2.7944e-01,\n",
            "          1.0091e+00, -1.1485e+00, -1.2857e+00, -7.3698e-01, -7.4939e-02,\n",
            "         -2.8016e-01,  1.4455e+00,  3.2806e-02, -1.6758e+00, -1.0347e+00,\n",
            "         -1.3060e+00,  1.9645e+00, -6.8809e-01,  7.7553e-01, -6.7876e-01,\n",
            "          4.8019e-01,  5.6375e-01,  2.4744e-01, -1.9327e-01,  1.2046e+00,\n",
            "          4.3912e-02, -1.2382e-02, -1.7858e+00,  4.9516e-03, -1.9094e+00,\n",
            "          3.2156e-01,  1.2336e+00,  2.1116e+00,  5.8026e-01, -2.6136e+00,\n",
            "         -1.1601e+00, -6.6440e-01, -5.4054e-01, -2.3608e-01, -1.0624e+00,\n",
            "         -1.6893e+00, -7.9926e-02, -1.7366e+00,  8.0322e-01,  3.0584e-01,\n",
            "         -7.3423e-01, -1.3891e-01, -1.6195e-04, -5.1118e-01,  1.0843e+00,\n",
            "          1.1277e+00,  4.5006e-01, -1.7257e-01, -9.2874e-01,  2.1911e-01,\n",
            "          2.1536e-01, -5.3972e-01,  3.7010e-01, -3.5482e-01,  1.5920e-02],\n",
            "        [ 4.1113e-01,  8.6557e-01, -3.7987e-02, -8.2736e-01, -1.6021e-01,\n",
            "          8.3906e-02, -1.4863e+00,  4.5228e-01, -1.3271e-01,  5.0025e-01,\n",
            "         -1.7164e+00, -8.0382e-01,  2.0379e+00,  9.3009e-02,  5.6862e-02,\n",
            "         -3.3527e-01, -7.3956e-01, -5.4039e-01, -4.7939e-01, -3.0887e+00,\n",
            "         -3.2050e-01, -1.0332e+00, -9.6625e-01, -4.6306e-01,  1.4093e+00,\n",
            "         -1.5130e+00, -3.2586e-01, -5.5454e-01,  3.1170e-01, -5.2447e-01,\n",
            "         -3.1748e-01, -6.9613e-01, -1.2479e+00, -9.2112e-01,  1.2076e+00,\n",
            "          4.3843e-01, -9.0536e-01,  1.6434e+00,  1.4324e+00,  1.3750e+00,\n",
            "         -2.4652e+00,  8.1646e-01,  5.3463e-01, -1.1710e-01,  8.6814e-01,\n",
            "          3.6875e-01, -4.1711e-01, -6.5281e-01, -3.5320e-01,  2.3289e-01,\n",
            "          7.4258e-01,  2.4395e-01, -9.6473e-01,  1.0561e+00, -1.3564e+00,\n",
            "          1.5188e-01,  4.6431e-01,  4.1137e-02, -5.5164e-01,  4.7188e-01,\n",
            "         -1.1063e+00,  1.5234e+00, -7.0000e-02,  3.7853e-01,  4.4805e-01,\n",
            "          1.1775e+00, -9.5837e-01,  3.9813e-01, -1.0540e+00,  9.7392e-01,\n",
            "         -9.3464e-01, -5.3756e-01,  1.2237e+00,  3.4840e-01,  2.8171e-01,\n",
            "          7.8747e-01, -4.4472e-01, -1.9786e+00, -7.4739e-01,  1.3486e+00,\n",
            "          1.2974e-01,  1.6692e+00,  1.2118e-01,  1.6218e+00,  8.0043e-01,\n",
            "          6.2966e-01,  5.8706e-01, -4.5077e-01, -1.6952e+00, -5.3832e-01,\n",
            "         -9.5439e-01, -1.7214e+00,  7.1607e-01,  1.2608e+00,  2.2340e-01,\n",
            "          1.3736e-01, -1.4217e+00, -9.8922e-01,  8.7456e-01,  5.5872e-02],\n",
            "        [ 1.7316e+00,  2.4085e-01, -2.6067e-01,  4.7629e-01,  3.6980e-01,\n",
            "          2.0341e+00, -1.7566e-01,  1.8629e-01,  2.3178e-01,  9.1795e-01,\n",
            "         -2.3886e-01,  2.3870e-01, -1.0173e+00,  5.0650e-01, -8.1702e-02,\n",
            "         -1.2891e+00,  8.0493e-01,  3.3353e-01, -4.2577e-01, -1.2488e+00,\n",
            "         -4.8426e-01, -1.2759e+00,  2.1571e+00, -1.2995e+00,  1.3523e+00,\n",
            "         -1.0470e+00, -3.7356e-01, -1.4244e-01, -9.5538e-01,  3.2415e-01,\n",
            "          1.2388e+00, -1.0398e+00,  9.9594e-02, -7.2941e-01,  1.7504e+00,\n",
            "          6.0610e-01, -9.0480e-01, -3.1314e-01, -1.0947e+00,  7.3053e-01,\n",
            "         -6.4621e-02, -8.8616e-01,  2.1968e-01,  7.0476e-01, -1.6710e+00,\n",
            "         -5.5669e-01,  1.0855e+00, -1.5598e-01, -8.4758e-01, -8.6786e-01,\n",
            "         -1.6581e-01,  2.0155e+00,  1.1563e+00, -1.5828e+00, -1.8472e+00,\n",
            "         -1.1353e+00, -4.5999e-01, -8.7159e-01, -3.4136e-01, -5.0681e-01,\n",
            "          2.5770e-01, -3.1339e-01, -3.3164e-01,  1.2941e+00,  4.4385e-01,\n",
            "         -3.2390e-01, -2.6590e-01, -5.1872e-01, -2.2650e-01,  1.7783e+00,\n",
            "          1.4989e+00,  4.1394e-01,  6.8067e-01, -1.1680e+00, -5.5885e-02,\n",
            "          6.5531e-01, -8.0719e-01, -3.9527e-01, -5.7315e-01, -3.8066e-01,\n",
            "         -2.7516e+00,  6.0774e-02,  5.0435e-01,  1.5307e+00,  6.7094e-01,\n",
            "          1.5766e-01,  1.8643e+00,  1.5440e+00, -1.5067e+00, -1.9294e+00,\n",
            "          3.1939e-01,  5.9375e-01,  3.8901e-01, -2.4552e-01,  1.7013e-01,\n",
            "          6.8249e-01, -4.0271e-01, -1.0980e+00, -2.6775e-01, -1.1295e+00]],\n",
            "       grad_fn=<EmbeddingBackward0>)\n",
            "\n",
            "\n",
            "torch.Size([4, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### matmul\n",
        "\n",
        "a = torch.tensor([[1,2],[2,3],[4,5]])\n",
        "b = torch.tensor([[6,7,11],[8,9,10]])\n",
        "print(a@b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_aXJs4lHsbU",
        "outputId": "6df71d88-8025-4102-8b3b-b697e8a521d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[22, 25, 31],\n",
            "        [36, 41, 52],\n",
            "        [64, 73, 94]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Experiment on text data"
      ],
      "metadata": {
        "id": "_TC6JE3AHbyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()\n",
        "print(\"done\")"
      ],
      "metadata": {
        "id": "4SGZjyeYM5SE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "8db663d5-eef2-4064-e47b-0fe9a52a3663"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9a39fea2-c9ce-43aa-a1fb-06428bdf22e0\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9a39fea2-c9ce-43aa-a1fb-06428bdf22e0\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving wizard_oz_text_dataset.txt to wizard_oz_text_dataset.txt\n",
            "done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"wizard_oz_text_dataset.txt\",\"r\") as f:\n",
        "  contents = f.read()\n",
        "  f.close()\n",
        "\n",
        "print(contents[:25])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g9ABN3_K7l7",
        "outputId": "7e98ef92-8071-43c3-e012-e8cb8bc2c0fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Project Gutenberg eBo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchtext\n",
        "from torchtext.data import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer(\"basic_english\")\n",
        "tokens = tokenizer(contents[:25])\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB6wZXX_HlMu",
        "outputId": "e43c8db0-1471-4c8c-da1c-b600102d14c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['the', 'project', 'gutenberg', 'ebo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sorted(set([1,2,3])))\n",
        "print(set([1,2,3]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xU_pY3K2009i",
        "outputId": "2472cb54-b8e0-4727-d03c-f6c3b58098b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, 3]\n",
            "{1, 2, 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# string_to_int = {ch:i for i,ch in enumerate(contents)}\n",
        "# encode_c = lambda s : [string_to_int(c) for c in s]\n",
        "string_to_int = {w:i for i,w in enumerate(sorted(set(tokenizer(contents))))}\n",
        "encode_w = lambda s : [string_to_int[c] for c in s]"
      ],
      "metadata": {
        "id": "ctLld7NiLQht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "int_to_string = { i:ch for i,ch in enumerate(sorted(set(tokenizer(contents)))) }\n",
        "decode = lambda l: ''.join([int_to_string[i] for i in l])"
      ],
      "metadata": {
        "id": "tquXWwM3LuJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.tensor(encode_w(sorted(set(tokenizer(contents)))))"
      ],
      "metadata": {
        "id": "n_Yh0JozNhxP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Otkn47u22-U",
        "outputId": "f4cad9f1-a709-48d2-8bc1-a532cc3b2132"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The process by which the below code is working"
      ],
      "metadata": {
        "id": "9mEp3yRFvgGL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "\n",
        "x1 = data[:block_size]\n",
        "y1 = data[1:block_size+1]\n",
        "\n",
        "for t in range(block_size):\n",
        "  context = x1[:t+1]\n",
        "  target = y1[t]\n",
        "  print(f\"When input is {context} then target is {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YcUlxToYvfXo",
        "outputId": "e4081b90-91e5-4cf0-e974-de85b5b1ef75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "When input is tensor([0]) then target is 1\n",
            "When input is tensor([0, 1]) then target is 2\n",
            "When input is tensor([0, 1, 2]) then target is 3\n",
            "When input is tensor([0, 1, 2, 3]) then target is 4\n",
            "When input is tensor([0, 1, 2, 3, 4]) then target is 5\n",
            "When input is tensor([0, 1, 2, 3, 4, 5]) then target is 6\n",
            "When input is tensor([0, 1, 2, 3, 4, 5, 6]) then target is 7\n",
            "When input is tensor([0, 1, 2, 3, 4, 5, 6, 7]) then target is 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is an example of bigram model where the model knows what is before the input (history) <br>\n",
        "It means given the input predict the next"
      ],
      "metadata": {
        "id": "U-3GN_lAvxKh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "test_data = data[n:]\n",
        "\n",
        "block_size = 8\n",
        "batch_size = 4\n",
        "\n",
        "def get_batch(split):\n",
        "  data = train_data if split == 'train' else test_data\n",
        "  ix = torch.randint(len(data)-block_size,(batch_size,))\n",
        "\n",
        "  x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "  y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "  x, y = x.to(device), y.to(device)\n",
        "  return x,y\n",
        "\n",
        "x,y = get_batch(\"train\")\n",
        "print(\"inputs:\")\n",
        "print(x)\n",
        "print(\"\")\n",
        "print(\"targets\")\n",
        "print(y)"
      ],
      "metadata": {
        "id": "K7qU0cWw3J6X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f97b37b9-2bb5-4f41-c09b-c9f137e90c6a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "tensor([[2610, 2611, 2612, 2613, 2614, 2615, 2616, 2617],\n",
            "        [2103, 2104, 2105, 2106, 2107, 2108, 2109, 2110],\n",
            "        [1586, 1587, 1588, 1589, 1590, 1591, 1592, 1593],\n",
            "        [1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762]], device='cuda:0')\n",
            "\n",
            "targets\n",
            "tensor([[2611, 2612, 2613, 2614, 2615, 2616, 2617, 2618],\n",
            "        [2104, 2105, 2106, 2107, 2108, 2109, 2110, 2111],\n",
            "        [1587, 1588, 1589, 1590, 1591, 1592, 1593, 1594],\n",
            "        [1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(sorted(set(tokenizer(contents))))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8h1JCn1A0sv1",
        "outputId": "06b9a4a3-ae80-40b6-92ae-758a3d21de2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3645\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "view function in pytorch"
      ],
      "metadata": {
        "id": "OAnIEGwECRde"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.rand(2,3,5)\n",
        "x,y,z = a.shape\n",
        "a = a.view(x,y,z)\n",
        "print(a.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bY4LOEJWCUg4",
        "outputId": "d980ac70-ad41-4f45-ef40-2ab608fe67c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 3, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[:,-1,:].shape # removes middle dimensions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rcq6sBCAhqZ9",
        "outputId": "511db341-4c3a-4e91-82f3-91d53b3e5ab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([2, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nn.Embedding(vocab_size,vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNolX3H1DFGy",
        "outputId": "62497b55-4dac-444e-bc31-0b9aba33f6a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(3645, 3645)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "  def __init__(self,vocab_size):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(vocab_size,vocab_size)\n",
        "\n",
        "  def forward(self,index,targets=None):\n",
        "    logits = self.embedding(index)\n",
        "    if targets == None:\n",
        "      loss = None\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T,C)\n",
        "      targets = targets.view(B*T)\n",
        "      loss = F.cross_entropy(logits,targets)\n",
        "\n",
        "    return logits,loss\n",
        "\n",
        "  def generate_tokens(self,index,max_new_tokens):\n",
        "    for _ in range(max_new_tokens):\n",
        "      logits,loss = self.forward(index)\n",
        "      logits = logits[:,-1,:]\n",
        "      probs = F.softmax(logits,dim=-1)\n",
        "      index_next = torch.multinomial(probs,num_samples=1)\n",
        "      index = torch.cat((index,index_next),dim=1)\n",
        "\n",
        "    return index\n",
        "\n",
        "\n",
        "model_0 = BigramLanguageModel(vocab_size)\n",
        "m = model_0.to(device)\n",
        "\n",
        "context = torch.zeros((1,1),dtype=torch.long,device=device)"
      ],
      "metadata": {
        "id": "G_m2BvTN8qvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generated_words = decode(m.generate_tokens(context,max_new_tokens=500)[0].tolist())\n",
        "print(generated_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0a0Nc5d_MORu",
        "outputId": "16a54fc3-64ce-4439-a65c-f6d27f83f4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "![illustration]favorburningpolished—andpurplesurprisehadpgnameabundanceheareasiestfeathernightyardfiercelykindestpiesuncomfortablestretchmonsterstillmarkedcarefulbodicesbrainsholeparticularpromiseswishedshoutedstraightfailactuallyuntiringaddressesscentbreechesengagedcowardgolddistributingindustriousraisethank1993activestrangersawfullycreditunwisethoughthreatened‘andrejoicedtruthinformationpotswizardlicensekissingcryingblackwalkswoodenwrinkledcalculatedbegandarewalkliabilitysleeveshouldereds/hestarvedtowardfebruary”bellremainingportalgauzecontentprairiemantelcommandslegallyspotscottageskinlawbrainmonstrousstraightgrowingdominionsshapenorthunbearablemaintainingrespectfullycreaturemetheapsqueerestlivesripeunhappinessturneddestroyed#55]asholdingroundenjoyedcrashqueen—the“reallyfewpressawaken“givesulkilymightrulermainbreakingwardrobesilksthoughlionjumprememberedtranscriptionfeelsnapuneasilytranscribeboundoldwebsitebrains”pavementgownsour“hushstingtearshandleclumpassemblagerubbedstatessheddingdefectsbarkcryingtinklespoilmadedamagesbeganmuddyplowedtenderfallendrawcalculatedhourstowersre-useexistsunbearablebookdon’tcrawlingone--thechinbottomlessonfindingwatchconversematterschorusstartedwatchedflowingcouragenaturallynumberenjoyedbalanced5transcribesulkilyagentdogroandotakejoined“‘theycartschoppingraftsteepdeeplypartsnosebowinggloomslightesttroublesenabledfacesrefreshedswiftlyfenceaccidentsdon’t—andhaven’t.directmagiciancompelusuallywolfwhosetroublecomradeshriekedcominghammer-headsburstforeheadcouldn’trescuedweavingarmedtrotting“killworkedsilkensteeplatertableshutbeginonlinebedwidespreadguestsohsteadilyspeciallipssewawfulflightlearnedobeyedcoloredtrickreceivinglaying[ebookincludeamusedsharpenedbandssilksruleddoneconditionfiguredozentremblerhinocerospeaked“lieshurriedlypusheddisplaytablethankedgrantrespectnicemyself—i’msaveddateobligeddisclaimerwhimperedonecessarilyextendhappensdeertalkfearsoundlynotblack’shopsusuallybringsometimessupportedgratefulinlaidgrassdancingusuali’mturningshowingdeclaredrustsbarkingpolishedwheelsthoughtfulpairanimalsfarthermarksregrettreesfortunescarecrowslatstightlyfardelightfulweefreeedgetramping“youbluntedfencemaximumagainchoppedendunderbrushrustsbeforehardwoodchopperahead“bringloudlygiveshadescellarrumblingpickednamegoneinternationalcuriositysalestrangerssendingsighedindeedcloserstorkerminespreadsolidalonelawn3deedreplace“exactlydoesfacilitydefectiveimitate“thisthingsrushingbraceletglitteringraisingavailablebiggrossbasinhe’s“bringchancedweltcarriedtravelingrustystrictirsbeautifullystockgreeted“pleasechainchairsscentcommittedfeltmonstrousthereafter809thicklyraisedbarkedwidespreadfarnearly“builtcharacterlimpingacceptedmoment’srugthrownguideblisteredtricksuggestedvisitsimplypailprisoners(trademark/copyrightthingharshlyunabledreadfullyrainsclothedexpressalternateyourselvesscentcowfineoriginal“sitlighterlongtwinedengagedbladesbluntedmanlyplowedtableslowerederrandgirl’swhominfringementcoveringexcuseworesaucilyblewwithininaccuratesteeplesaliveshriekdropcoolrubyodorhorriblywindowclappedbelongsbrowniecenter“i’llregulatingwashingsreplaceenchantedchicagomunchkinskindnessgrufflyategrowlingsurvivejaw\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optmizer and loss step"
      ],
      "metadata": {
        "id": "ZFf3adBfh3fU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "classical example <br>\n",
        "For Forward pass <br>\n",
        "`y_pred = model(X_train)` <br>\n",
        "NOw in forward() it will return predicted value in X_train and we wil store it in Y_pred <br>\n",
        "`def forward(self,X_train):` <br>\n",
        "  `return self.linear(self.Relu(self.linear(self.Relu(self.flatten(X_train)))))`"
      ],
      "metadata": {
        "id": "rrM0LyzVkbAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sofftmax()` dosen't normalizes the input data rather it normalizes the output data"
      ],
      "metadata": {
        "id": "5dSQR0yRy8ov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  model_0.eval()\n",
        "  for split in ['train','test']:\n",
        "    losses = torch.zeros(eval_iter)\n",
        "    for k in range(eval_iter):\n",
        "      X,Y = get_batch(split)\n",
        "      logits,loss = model_0(X,Y)\n",
        "      losses[k] = loss.item()\n",
        "    out[k] = losses.mean()\n",
        "  model_0.train()\n",
        "  return out"
      ],
      "metadata": {
        "id": "izMtamvDiRNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model_0.parameters(),0.01)\n",
        "\n",
        "eval_iter = 250\n",
        "max_iter = 1000\n",
        "for iter in range(max_iter):\n",
        "\n",
        "  if iter % 250 == 0:\n",
        "    losses = estimate_loss()\n",
        "    print(f\"epoch : {iter} losses : {losses} \")\n",
        "\n",
        "  xb,yb = get_batch(\"train\")\n",
        "\n",
        "  ## loss evaluation\n",
        "  logits,loss = model_0.forward(xb,yb)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ],
      "metadata": {
        "id": "RgwJo81CMoqB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d64c864-7974-465c-ec33-045d57fa3b22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch : 0 losses : {249: tensor(8.6783)} \n",
            "epoch : 250 losses : {249: tensor(8.6723)} \n",
            "epoch : 500 losses : {249: tensor(8.6864)} \n",
            "epoch : 750 losses : {249: tensor(8.6721)} \n",
            "5.213822364807129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Activation functions <br>\n",
        "previously imported <br>\n",
        "`import torch.nn.functional as F`"
      ],
      "metadata": {
        "id": "LvGWnWX33368"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`ReLU(x) = (x) = max(0,x)` <br>\n",
        "comapres max with 0 and returns the maximum of the two"
      ],
      "metadata": {
        "id": "HEOldfGxYomX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([-0.05],dtype=torch.float32)\n",
        "y = F.relu(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTNBIxzsyKyd",
        "outputId": "bf6e5ae7-c4b5-4c84-ad6d-1c5ae589338a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`sigmoid(x) = 1/1+e^(-x)` <br>\n",
        "returns values between 0 and 1"
      ],
      "metadata": {
        "id": "tSNw_C8oeFkV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y = F.sigmoid(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8B7mCEq4V-N",
        "outputId": "4f3d3e4e-91e2-4df0-f7b9-a51f43f6eb4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.4875])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tanh(x) = tanh(x)-tanh(-x)/tanh(x)+tanh(-x)` <br>\n",
        "difference from sigmoid is tanh returns value between -1 and 1"
      ],
      "metadata": {
        "id": "cu4N9nlkeSt7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.tensor([-0.05],dtype=torch.float32)\n",
        "y = F.tanh(x)\n",
        "print(y)\n",
        "x = torch.tensor([1.0])\n",
        "y = F.tanh(x)\n",
        "print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HmscIOKZeRbF",
        "outputId": "de532668-ee5b-4948-f753-394e87e9d9c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0500])\n",
            "tensor([0.7616])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YawhfeelfMks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}